{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import tree\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and display data shape\n",
    "imdb = pd.read_csv('IMDB_dataset.csv', nrows=10000)\n",
    "imdb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display last 5 rows of data\n",
    "# imdb.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summarize data\n",
    "# imdb.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check  distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_dist = imdb.groupby('sentiment').size()\n",
    "# labels = 'positive','negative',\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.set_title('Class Distribution', y=1.08)\n",
    "# ax.pie(class_dist, labels=labels, autopct='%1.1f%%',\n",
    "#        shadow=False, startangle=90)\n",
    "# ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "\n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "# Encode labels in column 'species'. \n",
    "imdb['sentiment']= label_encoder.fit_transform(imdb['sentiment']) \n",
    "\n",
    "imdb['sentiment'].unique() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie reviews vary in length. For example one movie review may contain 20 words while a second one 500 words.Below is a visualization of review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lengths = [len(i) for i in imdb['review']]\n",
    "# print(f'Max length of sentence: {max(lengths)}')\n",
    "# print(f'Average length of sentence: {np.mean(lengths)}')\n",
    "\n",
    "# sns.distplot(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words\n",
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors. Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(stop_words='english') \n",
    "\n",
    "# #`data` is an array of strings\n",
    "\n",
    "# data_cv = cv.fit_transform(imdb['review']) \n",
    " \n",
    "# print(data_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer supports counts of N-grams of words or consecutive characters. Once fitted, the vectorizer has built a dictionary of feature indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# #create a dictionary with feature names as keys and row elements as values\n",
    "# print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF\n",
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using TfidfTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tfidf_transformer = TfidfTransformer()\n",
    "# data_tfidf = tfidf_transformer.fit_transform(data_cv)\n",
    "# print(data_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data_tfidf, imdb['sentiment'], test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have our features, we can train a classifier to try to predict the category of a post. Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for word counts is the multinomial variant:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bayes_clf = MultinomialNB().fit(X_train, y_train)\n",
    "# bayes_clf_predicted = bayes_clf.predict(X_test)\n",
    "# print('Test accuracy: %.2f%%' % (np.mean(bayes_clf_predicted == y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Classification report for bag of words for Naive Bayes classification\n",
    "# bayes_clf_report=classification_report(y_test,bayes_clf_predicted,target_names=['Positive','Negative'])\n",
    "# print(bayes_clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bayes_clf_cm=confusion_matrix(y_test,bayes_clf_predicted,labels=[1,0])\n",
    "# print(bayes_clf_cm)\n",
    "\n",
    "# # Plot Confusion Matrix for Naive Bayes\n",
    "# bayes_clf_dm = pd.DataFrame(bayes_clf_cm, index = [i for i in ['positive', 'negative']],\n",
    "#               columns = [i for i in ['positive', 'negative']])\n",
    "# plt.figure(figsize = (10,7))\n",
    "# sns.heatmap(bayes_clf_dm, annot=True,cmap=\"OrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Linear SVC Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LinearSVC_clf = LinearSVC().fit(X_train, y_train)\n",
    "# LinearSVC_clf_predicted = LinearSVC_clf.predict(X_test)\n",
    "# print('Test accuracy: %.2f%%' % (np.mean(LinearSVC_clf_predicted == y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Classification report for bag of words \n",
    "# LinearSVC_clf_report=classification_report(y_test,LinearSVC_clf_predicted,target_names=['Positive','Negative'])\n",
    "# print(LinearSVC_clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LinearSVC_clf_cm=confusion_matrix(y_test,LinearSVC_clf_predicted,labels=[1,0])\n",
    "# print(LinearSVC_clf_cm)\n",
    "\n",
    "# # Plot Confusion Matrix\n",
    "# LinearSVC_clf_df_cm = pd.DataFrame(LinearSVC_clf_cm, index = [i for i in ['positive', 'negative']],\n",
    "#               columns = [i for i in ['positive', 'negative']])\n",
    "# plt.figure(figsize = (10,7))\n",
    "# sns.heatmap(LinearSVC_clf_df_cm, annot=True,cmap=\"OrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DecisionTree_clf = tree.DecisionTreeClassifier().fit(X_train, y_train)\n",
    "# DecisionTree_clf_predicted = DecisionTree_clf.predict(X_test)\n",
    "# print('Test accuracy: %.2f%%' % (np.mean(DecisionTree_clf_predicted == y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Classification report for bag of words \n",
    "# DecisionTree_clf_report=classification_report(y_test,DecisionTree_clf_predicted,target_names=['Positive','Negative'])\n",
    "# print(DecisionTree_clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree_clf_cm=confusion_matrix(y_test,DecisionTree_clf_predicted,labels=[1,0])\n",
    "# print(DecisionTree_clf_cm)\n",
    "\n",
    "# # Plot Confusion Matrix\n",
    "# DecisionTree_clf_df_cm = pd.DataFrame(DecisionTree_clf_cm, index = [i for i in ['positive', 'negative']],\n",
    "#               columns = [i for i in ['positive', 'negative']])\n",
    "# plt.figure(figsize = (10,7))\n",
    "# sns.heatmap(DecisionTree_clf_df_cm, annot=True,cmap=\"OrRd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "# #word cloud for positive review words\n",
    "# plt.figure(figsize=(10,10))\n",
    "# positive_text=imdb.review[1]\n",
    "# WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
    "# positive_words=WC.generate(positive_text)\n",
    "# plt.imshow(positive_words,interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Word cloud for negative review words\n",
    "# plt.figure(figsize=(10,10))\n",
    "# negative_text=imdb.review[5052]\n",
    "# WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
    "# negative_words=WC.generate(negative_text)\n",
    "# plt.imshow(negative_words,interpolation='bilinear')\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = simple_stemmer(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb['review']=imdb['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "# print(stop)\n",
    "\n",
    "#removing the stopword and tokenizing\n",
    "imdb['review'] = imdb.apply(lambda row: nltk.word_tokenize(row['review']), axis=1)\n",
    "imdb['review'] = imdb['review'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, review, ha, mention, watch, 1, Oz, episo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[A, wonder, littl, product, film, techniqu, ve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I, thought, thi, wa, wonder, way, spend, time...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[basic, famili, littl, boy, jake, think, zombi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[petter, mattei, love, time, money, visual, st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  [one, review, ha, mention, watch, 1, Oz, episo...          1\n",
       "1  [A, wonder, littl, product, film, techniqu, ve...          1\n",
       "2  [I, thought, thi, wa, wonder, way, spend, time...          1\n",
       "3  [basic, famili, littl, boy, jake, think, zombi...          0\n",
       "4  [petter, mattei, love, time, money, visual, st...          1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=19100, size=110, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "# Set values for various parameters\n",
    "feature_size = 110    # Word vector dimensionality  \n",
    "window_context = 50          # Context window size                                                                                    \n",
    "min_word_count = 3   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "fastxt = FastText(sentences=imdb.review[:], size=feature_size, window=window_context, \n",
    "                    min_count=min_word_count,sample=sample, sg=1, iter=5)\n",
    "print(fastxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keyvectors  and dictionary of words of FastText model\n",
    "\n",
    "word_keyvec = (fastxt.wv.vectors_vocab)\n",
    "\n",
    "word2index = {}\n",
    "for index, word in enumerate(fastxt.wv.index2word):\n",
    "    word2index[word] = index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_imdb_review = [] \n",
    "for sent in imdb.review:\n",
    "    temp_sent = []\n",
    "    for word in sent:\n",
    "            try:\n",
    "                temp_sent.append(word2index[word])\n",
    "            except KeyError:\n",
    "                      continue\n",
    "    new_imdb_review.append(temp_sent)\n",
    "    \n",
    "all_imdbreview_lens = [len(data) for data in new_imdb_review]\n",
    "max_review_len = (max(all_imdbreview_lens))\n",
    "padded_imdb_review = pad_sequences(new_imdb_review, maxlen=max_review_len)\n",
    "padded_imdb_review = np.array(padded_imdb_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  62925\n",
      "8000 train sequences\n",
      "2000 test sequences\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "maxlen = len(word_keyvec) + 1\n",
    "batch_size = 256\n",
    "embedding_dims = maxlen\n",
    "filters = 64\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 15\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=maxlen)\n",
    "tokenizer.fit_on_texts(imdb.review[:])\n",
    "list_tokenized_review = tokenizer.texts_to_sequences(imdb.review[:])\n",
    "word_index = tokenizer.word_index\n",
    "print('dictionary size: ', len(word_index))\n",
    "\n",
    "review = pad_sequences(list_tokenized_review, maxlen=maxlen)\n",
    "sentiment = imdb.sentiment[:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(list_tokenized_review, sentiment, test_size=0.20)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "# x_val = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights = [word_keyvec],\n",
    "                    trainable=True,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    \n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "            validation_data=(x_test, y_test))\n",
    "#                  callbacks=[TestCallback((x_test, y_test)), EarlyStopping(monitor='val_loss', patience=4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(new_imdb_review, imdb.sentiment, test_size=0.20)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_val), 'val sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights = [word_keyvec],\n",
    "                    trainable=True,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "            validation_data=(x_test, y_test))\n",
    "#                  callbacks=[TestCallback((x_test, y_test)), EarlyStopping(monitor='val_loss', patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(hist.history['loss'], lw=2.0, color='b', label='Training')\n",
    "plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='Validation')\n",
    "plt.title('CNN sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist.history['accuracy'], lw=2.0, color='b', label='Training')\n",
    "plt.plot(hist.history['val_accuracy'], lw=2.0, color='r', label='Validation')\n",
    "plt.title('CNN sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "model = Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights = [np.array(word_keyvec)],\n",
    "                    trainable=True,\n",
    "                    input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "            validation_data=(x_test, y_test))\n",
    "#                  callbacks=[TestCallback((x_test, y_test)), EarlyStopping(monitor='val_loss', patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(hist.history['loss'], lw=2.0, color='b', label='Training')\n",
    "plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='Validation')\n",
    "plt.title('CNN sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist.history['accuracy'], lw=2.0, color='b', label='Training')\n",
    "plt.plot(hist.history['val_accuracy'], lw=2.0, color='r', label='Validation')\n",
    "plt.title('LSTM sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test=pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "# df_test.head()\n",
    "# df_test[\"review\"]=df_test.review.apply(lambda x: clean_text(x))\n",
    "# df_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "# y_test = df_test[\"sentiment\"]\n",
    "# list_sentences_test = df_test[\"review\"]\n",
    "# list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "# X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "# prediction = model.predict(X_te)\n",
    "# y_pred = (prediction > 0.5)\n",
    "# from sklearn.metrics import f1_score, confusion_matrix\n",
    "# print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "# print('Confusion matrix:')\n",
    "# confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABELS = ['negative', 'positive']\n",
    "# sns.heatmap(cm, annot=True, xticklabels=LABELS, yticklabels=LABELS, fmt='g')\n",
    "# xl = plt.xlabel(\"Predicted\")\n",
    "# yl = plt.ylabel(\"Actuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4497    ,  0.97497225,  0.46112314, ...,  0.5811726 ,\n",
       "         0.28505516,  0.4657996 ],\n",
       "       [ 0.05410308,  0.18477237, -0.14086936, ..., -0.0512023 ,\n",
       "        -0.03936717, -0.1220993 ],\n",
       "       [ 0.89050376,  0.6236001 ,  0.27999282, ..., -0.15066345,\n",
       "        -0.5109988 , -0.7903654 ],\n",
       "       ...,\n",
       "       [-0.10815542, -0.05666108, -0.1758457 , ..., -0.12835437,\n",
       "         0.0601038 ,  0.01920075],\n",
       "       [-0.06119848,  0.34226292, -0.36258733, ..., -0.0468185 ,\n",
       "         0.14853662,  0.0020727 ],\n",
       "       [-0.03094644,  0.0110224 , -0.04127404, ..., -0.01928664,\n",
       "        -0.00266445, -0.00284141]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_keyvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19100\n"
     ]
    }
   ],
   "source": [
    "print(len(word_keyvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19100\n"
     ]
    }
   ],
   "source": [
    "print(len(list(fastxt.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19100\n"
     ]
    }
   ],
   "source": [
    "print(len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda18efcd73aca0417b869319057f882f5b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
